from pyspark.sql import SQLContext, Row

conf = {"es.resource" : "movies2/logs", "es.query" : "?q=name:bourne"}
movies = sc.newAPIHadoopRDD("org.elasticsearch.hadoop.mr.EsInputFormat",\
    "org.apache.hadoop.io.NullWritable", "org.elasticsearch.hadoop.mr.LinkedMapWritable", conf=conf)
    
movieRows = movies.map(lambda p: Row(id=int(p[1]['id']), name=p[1]['name']))
schemaMovie = sqlContext.createDataFrame(movieRows)
schemaMovie.registerTempTable("movies")
ids = sqlContext.sql("SELECT id FROM movies")
ids.collect()
[Row(id=2905809), Row(id=2156022), Row(id=2491302), Row(id=2491697), Row(id=3480649), Row(id=3480651), Row(id=3480654), Row(id=3480662), Row(id=3480674), Row(id=545213), Row(id=392194), Row(id=392195), Row(id=928484), Row(id=1290613), Row(id=1735318), Row(id=1855959), Row(id=1999204), Row(id=2655049), Row(id=3091562), Row(id=3480644), Row(id=3480648), Row(id=3480653), Row(id=3480659), Row(id=3480660), Row(id=3480665), Row(id=3480666), Row(id=3480667), Row(id=3480671), Row(id=382313), Row(id=898641), Row(id=2383133), Row(id=1239674), Row(id=1797090), Row(id=2979333), Row(id=3134961), Row(id=3480645), Row(id=3480650), Row(id=3480652), Row(id=3480655), Row(id=3480657), Row(id=3480664), Row(id=3480669), Row(id=3480672), Row(id=3480675), Row(id=3480676), Row(id=3480677), Row(id=361432), Row(id=392193), Row(id=678008), Row(id=2153446), Row(id=1971374), Row(id=1301759), Row(id=3511488), Row(id=3480647), Row(id=3480656), Row(id=3480661), Row(id=3480663), Row(id=3480668), Row(id=3480670), Row(id=3480678), Row(id=352076), Row(id=763678), Row(id=715525), Row(id=898301), Row(id=2198544), Row(id=2423043), Row(id=2521999), Row(id=2629009), Row(id=2655048), Row(id=2655050), Row(id=1301466), Row(id=1333089), Row(id=1338241), Row(id=1999203), Row(id=1999205), Row(id=3054845), Row(id=3000459), Row(id=3000458), Row(id=3480646), Row(id=3480658), Row(id=3480673), Row(id=3537907), Row(id=3717129), Row(id=862226), Row(id=862227), Row(id=898640)]

